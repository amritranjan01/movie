{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c13e81d5",
   "metadata": {},
   "source": [
    "Implementating And gate or gate ,Xor gate using perceptron from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6316c01b",
   "metadata": {},
   "source": [
    "# Study\n",
    "Logic gates are fundamental building blocks in digital circuits, performing logical operations on binary inputs. Perceptrons, inspired by biological neurons, are simple computational units that can be trained to perform binary classification tasks. This study aims to implement AND and OR gates using perceptrons, demonstrating the capability of these artificial neural network components in mimicking basic logical functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ef0f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing the and gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64fe5a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND(0, 1) = 0\n",
      "AND(1, 1) = 1\n",
      "AND(0, 0) = 0\n",
      "AND(1, 0) = 0\n"
     ]
    }
   ],
   "source": [
    "# importing Python library \n",
    "import numpy as np \n",
    "\n",
    "# define Unit Step Function \n",
    "def unitStep(v): \n",
    "\tif v >= 0: \n",
    "\t\treturn 1\n",
    "\telse: \n",
    "\t\treturn 0\n",
    "\n",
    "# design Perceptron Model \n",
    "def perceptronModel(x, w, b): \n",
    "\tv = np.dot(w, x) + b \n",
    "\ty = unitStep(v) \n",
    "\treturn y \n",
    "\n",
    "# AND Logic Function \n",
    "# w1 = 1, w2 = 1, b = -1.5 \n",
    "def AND_logicFunction(x): \n",
    "\tw = np.array([1, 1]) \n",
    "\tb = -1.5\n",
    "\treturn perceptronModel(x, w, b) \n",
    "\n",
    "# testing the Perceptron Model \n",
    "test1 = np.array([0, 1]) \n",
    "test2 = np.array([1, 1]) \n",
    "test3 = np.array([0, 0]) \n",
    "test4 = np.array([1, 0]) \n",
    "\n",
    "print(\"AND({}, {}) = {}\".format(0, 1, AND_logicFunction(test1))) \n",
    "print(\"AND({}, {}) = {}\".format(1, 1, AND_logicFunction(test2))) \n",
    "print(\"AND({}, {}) = {}\".format(0, 0, AND_logicFunction(test3))) \n",
    "print(\"AND({}, {}) = {}\".format(1, 0, AND_logicFunction(test4))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5606890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "implementing the or gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d2fc568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR(0, 1) = 1\n",
      "OR(1, 1) = 1\n",
      "OR(0, 0) = 0\n",
      "OR(1, 0) = 1\n"
     ]
    }
   ],
   "source": [
    "# importing Python library \n",
    "import numpy as np \n",
    "\n",
    "# define Unit Step Function \n",
    "def unitStep(v): \n",
    "\tif v >= 0: \n",
    "\t\treturn 1\n",
    "\telse: \n",
    "\t\treturn 0\n",
    "\n",
    "# design Perceptron Model \n",
    "def perceptronModel(x, w, b): \n",
    "\tv = np.dot(w, x) + b \n",
    "\ty = unitStep(v) \n",
    "\treturn y \n",
    "\n",
    "# OR Logic Function \n",
    "# w1 = 1, w2 = 1, b = -0.5 \n",
    "def OR_logicFunction(x): \n",
    "\tw = np.array([1, 1]) \n",
    "\tb = -0.5\n",
    "\treturn perceptronModel(x, w, b) \n",
    "\n",
    "# testing the Perceptron Model \n",
    "test1 = np.array([0, 1]) \n",
    "test2 = np.array([1, 1]) \n",
    "test3 = np.array([0, 0]) \n",
    "test4 = np.array([1, 0]) \n",
    "\n",
    "print(\"OR({}, {}) = {}\".format(0, 1, OR_logicFunction(test1))) \n",
    "print(\"OR({}, {}) = {}\".format(1, 1, OR_logicFunction(test2))) \n",
    "print(\"OR({}, {}) = {}\".format(0, 0, OR_logicFunction(test3))) \n",
    "print(\"OR({}, {}) = {}\".format(1, 0, OR_logicFunction(test4))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b497dd24",
   "metadata": {},
   "source": [
    "implementing and and or gate both in a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e0ad844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR predictions:\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 1\n",
      "[1 1] -> 1\n",
      "\n",
      "AND predictions:\n",
      "[0 0] -> 0\n",
      "[0 1] -> 0\n",
      "[1 0] -> 0\n",
      "[1 1] -> 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size, learning_rate=0.1, epochs=100):\n",
    "        self.weights = np.random.rand(input_size)\n",
    "        self.bias = np.random.rand()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def step_function(self, x):\n",
    "        return 1 if x >= 0 else 0\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return self.step_function(np.dot(inputs, self.weights) + self.bias)\n",
    "\n",
    "    def train(self, training_inputs, labels):\n",
    "        for epoch in range(self.epochs):\n",
    "            for inputs, label in zip(training_inputs, labels):\n",
    "                prediction = self.predict(inputs)\n",
    "                error = label - prediction\n",
    "                self.weights += self.learning_rate * error * inputs\n",
    "                self.bias += self.learning_rate * error\n",
    "\n",
    "# OR function training\n",
    "or_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "or_labels = np.array([0, 1, 1, 1])\n",
    "\n",
    "or_perceptron = Perceptron(input_size=2)\n",
    "or_perceptron.train(or_inputs, or_labels)\n",
    "\n",
    "# AND function training\n",
    "and_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "and_labels = np.array([0, 0, 0, 1])\n",
    "\n",
    "and_perceptron = Perceptron(input_size=2)\n",
    "and_perceptron.train(and_inputs, and_labels)\n",
    "\n",
    "# Testing the trained perceptrons\n",
    "print(\"OR predictions:\")\n",
    "for inputs in or_inputs:\n",
    "    print(f\"{inputs} -> {or_perceptron.predict(inputs)}\")\n",
    "\n",
    "print(\"\\nAND predictions:\")\n",
    "for inputs in and_inputs:\n",
    "    print(f\"{inputs} -> {and_perceptron.predict(inputs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63641aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## implemeting the xor gate by using the perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49ee0bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR(0, 1) = 1\n",
      "XOR(1, 1) = 0\n",
      "XOR(0, 0) = 0\n",
      "XOR(1, 0) = 1\n"
     ]
    }
   ],
   "source": [
    "# importing Python library\n",
    "import numpy as np\n",
    "\n",
    "# define Unit Step Function\n",
    "def unitStep(v):\n",
    "\tif v >= 0:\n",
    "\t\treturn 1\n",
    "\telse:\n",
    "\t\treturn 0\n",
    "\n",
    "# design Perceptron Model\n",
    "def perceptronModel(x, w, b):\n",
    "\tv = np.dot(w, x) + b\n",
    "\ty = unitStep(v)\n",
    "\treturn y\n",
    "\n",
    "# NOT Logic Function\n",
    "# wNOT = -1, bNOT = 0.5\n",
    "def NOT_logicFunction(x):\n",
    "\twNOT = -1\n",
    "\tbNOT = 0.5\n",
    "\treturn perceptronModel(x, wNOT, bNOT)\n",
    "\n",
    "# AND Logic Function\n",
    "# here w1 = wAND1 = 1, \n",
    "# w2 = wAND2 = 1, bAND = -1.5\n",
    "def AND_logicFunction(x):\n",
    "\tw = np.array([1, 1])\n",
    "\tbAND = -1.5\n",
    "\treturn perceptronModel(x, w, bAND)\n",
    "\n",
    "# OR Logic Function\n",
    "# w1 = 1, w2 = 1, bOR = -0.5\n",
    "def OR_logicFunction(x):\n",
    "\tw = np.array([1, 1])\n",
    "\tbOR = -0.5\n",
    "\treturn perceptronModel(x, w, bOR)\n",
    "\n",
    "# XOR Logic Function\n",
    "# with AND, OR and NOT \n",
    "# function calls in sequence\n",
    "def XOR_logicFunction(x):\n",
    "\ty1 = AND_logicFunction(x)\n",
    "\ty2 = OR_logicFunction(x)\n",
    "\ty3 = NOT_logicFunction(y1)\n",
    "\tfinal_x = np.array([y2, y3])\n",
    "\tfinalOutput = AND_logicFunction(final_x)\n",
    "\treturn finalOutput\n",
    "\n",
    "# testing the Perceptron Model\n",
    "test1 = np.array([0, 1])\n",
    "test2 = np.array([1, 1])\n",
    "test3 = np.array([0, 0])\n",
    "test4 = np.array([1, 0])\n",
    "\n",
    "print(\"XOR({}, {}) = {}\".format(0, 1, XOR_logicFunction(test1)))\n",
    "print(\"XOR({}, {}) = {}\".format(1, 1, XOR_logicFunction(test2)))\n",
    "print(\"XOR({}, {}) = {}\".format(0, 0, XOR_logicFunction(test3)))\n",
    "print(\"XOR({}, {}) = {}\".format(1, 0, XOR_logicFunction(test4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33f74d7",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "This study demonstrates the successful implementation of AND and OR gates using perceptrons, showcasing the ability of artificial neural networks to emulate basic logical operations. Further research can explore more complex logic gates and the application of more sophisticated neural network architectures for broader computational tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
